---
title: "K-Means Clustering"
output:
  html_document:
    df_print: paged
---

## Introduction to K-Means Clustering

The first step in k-means clustering is determining the desired number of 
clusters, this is the k in k-means clustering. This number of clusters can be 
defined prior to analysis or estimated.

The first step in the actual k-means clustering algorithm is to initialize k 
points at random positions in the feature space. These points are referred to 
as the cluster centroids.

For each observation, the distance to each of the centroids is calculated. The 
observation is then assigned to the centroid which is the closest. In k-means 
clustering, the distance metric used is always Euclidean distance. This results 
in an initial assignment of each observation to a cluster.

The k points, that were initially placed at random in the feature space, are 
now centered at the center of the cluster to which they belong. Again, for each 
observation the distance to each centroid is calculated, and the observation is 
assigned to the centroid which is closest.

This process repeats itself until the positions of the centroids stabilize, and 
the observations are no longer reassigned to another group (cluster).

```{r}
# Load packages
library(tidyverse)
library(cluster)
```

```{r}
# Data frame with player positions
players <- data.frame(
  x = c(-1, -2, 8, 7, -12, -15, -13, 15, 21, 12, -25, 26),
  y = c(1, -3, 6, -8, 8, 0, -10, 16, 2, -15, 1, 0)
)
```

```{r}
# Build a model
model_km2 <- kmeans(players, centers = 2)
```

```{r}
# Extract cluster assignments
clust_assignments <- model_km2$cluster
```

```{r}
# Add the cluster assignments to the data frame
clustered_players <- players %>%
  dplyr::mutate(cluster = clust_assignments)
```

```{r}
# Visualize the clustered data points
ggplot(clustered_players, aes(x, y, color = factor(cluster))) +
  geom_point()
```

## Evaluating the Value of K by Eye

Using the elbow method, the value of k can be evaluated. The within-cluster sum 
of squares is calculated for different values of k. As the value for k 
increases, the within-cluster sum of squares decreases. This is natural. As you 
increase the number of clusters, the number of observations within each cluster 
decreases and the variation between the observations within a cluster (in other 
words, the distance from each observation to the centroid) decreases. An elbow 
plot can help in determining the optimal number of clusters, as this is often 
not known.

```{r}
# Build different k-means models
tot_withinss <- map_dbl(1:10,  function(k){
  model <- kmeans(x = players, centers = k)
  model$tot.withinss
})

# Generate a data frame
elbow_df <- data.frame(
  k = 1:10 ,
  tot_withinss = tot_withinss
)
```

```{r}
# Visualize the elbow plot
ggplot(elbow_df, aes(k, tot_withinss)) +
  geom_line() +
  scale_x_continuous(breaks = 1:10)
```

This plot shows an "elbow" at a k equaling 2. At this point, there is a sharp 
decrease in the absolute value of the slope of the curve.

## Silhouette Analysis

Silhouette analysis can be used to assess how well an observation fits into the 
cluster it was assigned to (observation-level performance), and it can be used 
as an additional method to estimate the best value for k.

In silhouette analysis, the silhouette width is calculated for every 
observation within the data set. This silhouette width consists of two parts, 
i.e., the within-cluster distance, C(i), and the closest-neighbor distance, 
N(i).

The within-cluster distance C(i) is calculated as the average of the Euclidean 
distances between the observation of interest to every other observation within 
the same cluster.

For the closest-neighbor distance N(i), the average of the Euclidean distances 
between the observation of interest and all of the other observations in 
another cluster are calculated, and this for every cluster except for the one 
the observation of interest belongs to. The smallest of these average distances 
forms the closest-neighbor distance N(i).

The silhouette width S(i) equals:

- if C(i) < N(i): S(i) = 1 - [C(i) / N(i)]
- if C(i) = N(i): S(i) = 0
- if C(i) > N(i): S(i) = [N(i) / C(i)] - 1

The silhouette width S(i) is greater than or equal to -1 and less than or equal 
to 1. A silhouette width that equals 1 indicates that the observation fits the 
cluster it is assigned to very well. A silhouette width that equals 0 indicates 
that the observation lies on the border between two clusters, the cluster it 
was assigned to and a neighboring cluster. A silhouette width that equals -1 
indicates that the observation would be a better fit in the neighboring 
cluster.

```{r}
# Generate a k-means model with a k = 2
pam_k2 <- pam(players, k = 2)

# Plot the silhouette visual for the model
plot(silhouette(pam_k2))
```

```{r}
# Generate a k-means model with a k = 3
pam_k3 <- pam(players, k = 3)

# Plot the silhouette visual for the model
plot(silhouette(pam_k3))
```

Silhouette analysis can be run for different values of k. The value of k that 
gives the highest average silhouette width can be considered the most optimal 
value of k.








